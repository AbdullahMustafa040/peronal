# -*- coding: utf-8 -*-
"""ResearchPaperWork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TuIe_LieWSHRMrBABewtDkKPCi6bvxZB
"""

!pip install pandas numpy tensorflow scikit-learn

"""# New Section"""

import requests

# URL of the raw CSV file
url = "https://raw.githubusercontent.com/AbdullahMustafa040/peronal/refs/heads/main/Research_Paper_%20Dataset3.csv"
try:
    response = requests.get(url)
    response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)

    # Save the content
    with open("Research_Paper_Dataset3.csv", "wb") as file:
        file.write(response.content)
    print("File downloaded successfully!")
except requests.exceptions.RequestException as e:
    print(f"Error downloading the file: {e}")

import pandas as pd

# Load the CSV file
data = pd.read_csv("Research_Paper_Dataset3.csv")
print(data.head())

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical

# Load Dataset
# Assuming your dataset is in a CSV file with 'review' and 'sentiment' columns

# Data Preprocessing
def preprocess_text(text):
    # Remove special characters, numbers, and convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower().strip()
    return text





data['Review'] = data['Review'].apply(preprocess_text)

# Label Encoding (Positive -> 2, Neutral -> 1, Negative -> 0)
unique_categories = data['Category'].unique()
print("Unique Categories:", unique_categories)

label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

# Tokenization and Padding
max_words = 10000  # Vocabulary size
max_len = 100      # Maximum review length

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['Review'])

X = tokenizer.texts_to_sequences(data['Review'])
X = pad_sequences(X, maxlen=max_len)

y = to_categorical(data['Category'])

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# RNN Model (LSTM)
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(y_train.shape[1], activation='softmax')  # 3 classes: Positive, Neutral, Negative
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

# Train the Model
epochs = 10
batch_size = 16
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)

# Prediction on New Reviews
def predict_review(review):
    review = preprocess_text(review)
    seq = tokenizer.texts_to_sequences([review])
    padded = pad_sequences(seq, maxlen=max_len)
    pred = model.predict(padded)
    return label_encoder.inverse_transform([np.argmax(pred)])

# Example
new_review = ""
print("Predicted Sentiment:", predict_review(new_review))

type(X_test)

X_test[0]

len(X_test[0])

pred=model.predict(X_test[0].reshape(1, -1))

label_encoder.inverse_transform([np.argmax(pred)])

y_test[0]

to_categorical(data['Category'])

def predict_review(review):
    review = preprocess_text(review)
    seq = tokenizer.texts_to_sequences([review])
    padded = pad_sequences(seq, maxlen=max_len)
    print(padded)
    # pred = model.predict(padded)
    # return label_encoder.inverse_transform([np.argmax(pred)])

# Example
new_review = ""
print("Predicted Sentiment:", predict_review(new_review))

unique_categories = data['Category'].unique()

unique_categories

data2 = pd.read_csv("/content/research_paper_dataset.csv")

unique_categories = data2['Category'].unique()

unique_categories

predictions = model.predict(X_test)
predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))

actual_labels = label_encoder.inverse_transform(np.argmax(y_test, axis=1))

reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

def reconstruct_text(sequence):
    return " ".join([reverse_word_map.get(i, "") for i in sequence if i != 0])

original_texts = [reconstruct_text(seq) for seq in X_test]

results_df = pd.DataFrame({'text': original_texts, 'actual output': actual_labels, 'predicted output': predicted_labels})

results_df.to_csv('results.csv')

model.save_weights("Bi_lstm_model_acc_83_val-acc_68..weights.h5")

data.to_csv('/content/research_paper_dataset2.csv')

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical

# Load Dataset
# Assuming your dataset is in a CSV file with 'review' and 'sentiment' columns
data = pd.read_csv("/content/Research_Paper_Dataset2.csv" , encoding='latin-1')
data=data.dropna()

# Data Preprocessing
def preprocess_text(text):
    # Remove special characters, numbers, and convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower().strip()
    return text





data['Review'] = data['Review'].apply(preprocess_text)

# Label Encoding (Positive -> 2, Neutral -> 1, Negative -> 0)
unique_categories = data['Category'].unique()
print("Unique Categories:", unique_categories)



data = pd.read_csv("/content/Research_Paper_Dataset.csv" , encoding='latin-1')
data=data.dropna()
data[data['Category'] == 'ne']

"""Dataset Check Code"""

!pip install pandas numpy tensorflow scikit-learn

import pandas as pd
import numpy as np
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical

# Load Dataset
# Assuming your dataset is in a CSV file with 'review' and 'sentiment' columns
data = pd.read_csv("/content/Research_Paper_Dataset.csv" , encoding='latin-1')
data=data.dropna()
print(data.head())

# Data Preprocessing
def preprocess_text(text):
    # Remove special characters, numbers, and convert to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower().strip()
    return text


data['Review'] = data['Review'].apply(preprocess_text)

# Label Encoding (Positive -> 2, Neutral -> 1, Negative -> 0)
unique_categories = data['Category'].unique()
print("Unique Categories:", unique_categories)

label_encoder = LabelEncoder()
data['Category'] = label_encoder.fit_transform(data['Category'])

# Tokenization and Padding
max_words = 10000  # Vocabulary size
max_len = 100      # Maximum review length
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(data['Review'])

X = tokenizer.texts_to_sequences(data['Review'])
X = pad_sequences(X, maxlen=max_len)

y = to_categorical(data['Category'])

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# RNN Model (LSTM)
model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(y_train.shape[1], activation='softmax')  # 3 classes: Positive, Neutral, Negative
])

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

# Train the Model
epochs = 10
batch_size = 16
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=batch_size)


predictions = model.predict(X_test)
predicted_labels = label_encoder.inverse_transform(np.argmax(predictions, axis=1))

actual_labels = label_encoder.inverse_transform(np.argmax(y_test, axis=1))

reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

def reconstruct_text(sequence):
    return " ".join([reverse_word_map.get(i, "") for i in sequence if i != 0])

original_texts = [reconstruct_text(seq) for seq in X_test]

results_df = pd.DataFrame({'text': original_texts, 'actual output': actual_labels, 'predicted output': predicted_labels})

results_df.to_csv('results.csv')

"""#asd

"""

!pip install groq

import pandas as pd
df = pd.read_csv("/content/mismatched_reviews.csv", encoding="ISO-8859-1")

df.head()



import pandas as pd
import time
from groq import Groq

# Initialize Groq Client
client = Groq(api_key="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")

# Load CSV File
df = pd.read_csv("/content/mismatched_reviews.csv", encoding="ISO-8859-1")

# Function to get sentiment from Grok API
def get_sentiment(review):
    prompt = (
        f"Classify the sentiment of this review as strictly one of the following: "
        f"'Positive', 'Negative', or 'Neutral'. No other response is allowed.\n\n"
        f"I don't need any action or any other thing related to the thinking process I just need one word Which is category"
        f"For context it is a Roman urdu"
        f"just give category nothing else"
        f"Review: \"{review}\""
    )

    try:
        response = client.chat.completions.create(
            model="deepseek-r1-distill-llama-70b",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_completion_tokens=1000,
            top_p=1,
            stop=None,
        )

        category = response.choices[0].message.content.strip().split()[-1]
        print(category)
        # Ensure the response is only one of the expected categories
        return category if category in ["Positive", "Negative", "Neutral"] else "Unknown"

    except Exception as e:
        print(f"Error: {e}")
        return "Error"

# Process each review one by one
for i in range(len(df)):
    review = df.iloc[i]["Review"]  # Extract the review
    df.loc[i, "Category_2.0"] = get_sentiment(review)  # Store the result in "Category_2.0"
    #time.sleep(2)  # Pause to avoid API limits

# Save updated DataFrame
df.to_csv("/content/mismatched_reviews_with_predictions.csv", index=False)

print("Processing complete! Predictions saved.")

df.head()

get_sentiment("1 ghanta agay karnay say kuch nai hua bal kay insan ko pareshaniyan uthani pari .")

# Save updated DataFrame
df.to_csv("/content/mismatched_reviews_with_predictions.csv", index=False)

print("Processing complete! Predictions saved.")

prompt = (
        f"Classify the sentiment of this review as strictly one of the following: "
        f"'Positive', 'Negative', or 'Neutral'. No other response is allowed.\n\n"
        f"I don't need any action or any other thing related to the thinking process I just need one word Which is category"
        f"For context it is a Roman urdu"
        f"Review: \"{review}\""
    )

print(prompt)

import numpy as np
import pandas as pd
import spacy
import codecs, sys
import random
from collections import Counter
import pickle

class RomanTokenizer:
    def __init__(self, dictionary_path='/content/dictionary.pkl'):
        self.unlp = spacy.blank('ur')
        self.dictionary = self.loadFromPickle(dictionary_path)

    def loadFromPickle(self, filepath):
        with open(filepath, 'rb') as f:
            return pickle.load(f)

    def roman_tokenizer(self, sentence):
        # Lowering
        sentence = sentence.lower()

        # Default Parameters
        tokens = []
        punctuation = '''!()%\n٪-;۔،:\n\/'"\,“./؟_ء'''

        sentence = self.unlp(sentence)

        # Iterating Word By Word
        for word in sentence:
            word = str(word)
            # Iterating Index By Index
            for index in word:
                if index in punctuation:
                    word = word.replace(index, '')

            # Removing Any Remaining Special Characters
            if word != "\n" and word != "\r" and word != " " and word != '' and word != " \r" and word != '‘' and word != '’':
                tokens.append(word)

        bi_tokens = []
        for i in range(len(tokens)):
            if i + 1 < len(tokens):
                a = tokens[i]
                b = tokens[i + 1]

                if a + ' ' + b in self.dictionary:
                    index = tokens.index(a)
                    tokens.remove(a)
                    tokens.remove(b)
                    tokens.insert(index, a + ' ' + b)

        return tokens

if __name__ == "__main__":
    # Example paragraph
    paragraph = "kuch nahi hota..meri ma ka hai ausman card unko 2 baar hard atek ayaa tha ilaaj ke liy pase nahi the doctor ny bola is card se sarkar pase nahi milte"

    # Create an instance of RomanTokenizer
    tokenizer = RomanTokenizer()

    # Tokenize the paragraph
    tokens = tokenizer.roman_tokenizer(paragraph)

    # Print the tokens
    print("Tokens:", tokens)

from google.colab import drive
drive.mount('/content/drive')